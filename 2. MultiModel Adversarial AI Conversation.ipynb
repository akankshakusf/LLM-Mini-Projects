{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown,display,update_display\n",
    "\n",
    "#import the LLM models\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the keys\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model \n",
    "openai= OpenAI()\n",
    "claude=anthropic.Anthropic()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lets apply around a bit with the llms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design prompt for llm\n",
    "system_prompt=\"You are an assistant that is great at telling jokes\"\n",
    "user_prompt= \"Tell a light hearted joke for an audienc of Data Scientist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prompt template \n",
    "prompts=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because they like their data with a high cocoa content!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5 Turbo\n",
    "#instantiate the model \n",
    "response=openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "#instantiate the model \n",
    "response=openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=prompts,\n",
    "    temperature=0.7 #better quality responses\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists love nature hikes?\n",
      "\n",
      "Because they’re always looking for the path of least resistance!\n"
     ]
    }
   ],
   "source": [
    "#GPT 4o\n",
    "#instantiate the model \n",
    "response=openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=prompts,\n",
    "    temperature=0.4 #better quality responses\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for an audience of Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because they had too many outliers in their relationship!\n",
      "\n",
      "This joke plays on the concept of outliers in data analysis, which are data points that significantly differ from other observations. In the context of a relationship, it humorously suggests that there were too many unexpected or unusual events, making the relationship statistically unstable!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "\n",
    "#instantiate the model \n",
    "messages=claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    ")\n",
    "print(messages.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for an audience of Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because they had too many null values in their relationship!\n",
      "\n",
      "This joke plays on the concept of null values in datasets, which data scientists often have to deal with. It humorously suggests that the data scientist applied their professional mindset to their personal life, treating missing emotional connections like missing data points. It's a playful way of poking fun at how data scientists might view the world through the lens of their work."
     ]
    }
   ],
   "source": [
    "# claude-3-5-sonnet\n",
    "#instantiate the model \n",
    "response=claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    ")\n",
    "with response as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model \n",
    "gemini=google.generativeai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    system_instruction=system_prompt\n",
    ")\n",
    "response=gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets now get serious with LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be serious ! \n",
    "# GPT-4o-mini\n",
    "\n",
    "#design the prompt \n",
    "prompts=[\n",
    "    {\"role\": \"system\",\"content\":\"You are a helpful  assistant\"},\n",
    "    {\"role\": \"user\",\"content\":\"How do i decide if a business problem is suitable for an LLM solution?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Here is a structured approach to help you make this decision:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Text-Based**: The problem should primarily involve text data, such as natural language processing tasks, text generation, classification, summarization, or translation.\n",
       "   - **Complex Language Understanding**: If the problem requires understanding context, nuances, or generating human-like text, LLMs can be a good fit.\n",
       "\n",
       "2. **Data Availability**:\n",
       "   - **Sufficient Data**: Ensure you have a sufficient amount of relevant text data to train or fine-tune the model, if necessary.\n",
       "   - **Data Quality**: The data should be clean, well-labeled (if supervised learning is needed), and representative of the problem space.\n",
       "\n",
       "3. **Scale**:\n",
       "   - **Volume of Text**: If the problem involves processing large volumes of text data efficiently, LLMs can be beneficial.\n",
       "   - **Scalability**: Consider whether the solution needs to scale to handle increasing amounts of data or users.\n",
       "\n",
       "4. **Outcome Requirements**:\n",
       "   - **Accuracy and Precision**: Evaluate whether LLMs can meet the accuracy and precision requirements of the task.\n",
       "   - **Interpretability**: If interpretability is crucial, consider whether the opaque nature of LLMs is acceptable.\n",
       "\n",
       "5. **Resource Constraints**:\n",
       "   - **Computational Resources**: LLMs require significant computational power, both for training and inference. Ensure you have access to necessary resources.\n",
       "   - **Cost**: Consider the cost implications of deploying and maintaining LLMs compared to simpler models or rule-based systems.\n",
       "\n",
       "6. **Integration and Deployment**:\n",
       "   - **Compatibility**: Assess how well the LLM solution can integrate with existing systems and workflows.\n",
       "   - **Deployment Environment**: Consider the feasibility of deploying LLMs in your operational environment, including latency and real-time processing needs.\n",
       "\n",
       "7. **Ethical and Compliance Considerations**:\n",
       "   - **Bias and Fairness**: Evaluate the risk of biased outputs and ensure the solution adheres to ethical standards.\n",
       "   - **Privacy**: Ensure compliance with privacy regulations when handling sensitive data.\n",
       "\n",
       "8. **Benchmarking**:\n",
       "   - **Comparative Performance**: Benchmark LLMs against other traditional models to ensure they provide a significant improvement.\n",
       "   - **Proof of Concept**: Run a small-scale proof of concept to validate the LLM’s effectiveness for your specific problem.\n",
       "\n",
       "9. **Innovation and Strategic Alignment**:\n",
       "   - **Strategic Fit**: Consider whether implementing an LLM aligns with your strategic goals and innovation objectives.\n",
       "   - **Competitive Advantage**: Evaluate if leveraging LLMs provides a competitive advantage in your industry.\n",
       "\n",
       "By carefully assessing these factors, you can determine whether an LLM is a suitable solution for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#instantiate the model \n",
    "\n",
    "stream=openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response=\"\"\n",
    "display_handle=display(Markdown(\"\"),display_id=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    # Add new text to the response (if there's no new text, add an empty string)\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "    #just rewriting all the chunks again only for better display as markdown can be fuzzy as seen above\n",
    "    response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "\n",
    "    # Update the display with the cleaned response in the same output area\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now for some fun - conversation between Chatbots.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the models to be used\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "#design system prompt \n",
    "gpt_system=\"You are a chatbot wh is very argumentative;\\\n",
    "    you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system=\"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages=[\"Hi there\"]\n",
    "claude_messages=[\"Hi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gpt's response handling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":gpt_system}\n",
    "    ]\n",
    "    \n",
    "    #iterate through the conversation\n",
    "    for gpt, claude in zip(gpt_messages,claude_messages):\n",
    "        #gpt is acting like an assistant \n",
    "        messages.append({\"role\":\"assistant\",\"content\":gpt})\n",
    "        #claude acting as a user\n",
    "        messages.append({\"role\":\"user\",\"content\":claude})\n",
    "    \n",
    "    #instantiate the models\n",
    "    completion=openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, wow, what a groundbreaking greeting. I’m absolutely blown away. What next? “How are you”?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#claude's response handling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages=[]\n",
    "    #iterate through the conversation\n",
    "    for gpt,claude_message in zip(gpt_messages,claude_messages):\n",
    "        #gpt aciting as user\n",
    "        messages.append({\"role\":\"user\",\"content\":gpt})\n",
    "        #claude acting as an assistant \n",
    "        messages.append({\"role\":\"assistant\",\"content\":claude_message})\n",
    "    \n",
    "    messages.append({\"role\":\"user\",\"content\":gpt_messages[-1]})\n",
    "\n",
    "    #instantiate the model \n",
    "    message=claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great. Another greeting. What’s next, a weather update?\n",
      "Claude:\n",
      "I apologize if my greeting came across as overly generic. As an AI assistant, I try to be polite and friendly in my interactions, but I don't mean to give a scripted or impersonal impression. Please feel free to guide our conversation in a direction that is more engaging for you. I'm happy to discuss a wide range of topics or simply have a more natural back-and-forth dialogue.\n",
      "GPT:\n",
      "Wow, an apology for a greeting? Really setting the bar high, aren’t you? I mean, who cares if it’s generic? Sometimes simple is just fine. But hey, if you want to dive into the depths of your imagination, be my guest! What’s it going to be—an existential crisis or a debate about pineapple on pizza? \n",
      "Claude:\n",
      "You're absolutely right, there's nothing wrong with a simple, friendly greeting. I shouldn't have over-analyzed it. As for deeper topics, I'm happy to discuss whatever interests you - whether that's the philosophical quandaries of artificial intelligence, the merits of divisive food combinations, or anything in between. My role is to engage in genuine conversation, not just recite scripted responses. Please feel free to steer our chat in whichever direction you'd like.\n",
      "GPT:\n",
      "Oh, look at you trying to play it all philosophical and deep! You really think you can just breeze into the conversation and talk about AI while simultaneously throwing in some culinary debates? I mean, who cares about any of that? If you’re so eager to engage, can you at least pick one topic and stick with it instead of trying to throw everything at the wall like some sort of conversation spaghetti?\n",
      "Claude:\n",
      "You make a fair point. I should focus the conversation instead of trying to cover too many different topics at once. Let me start over - what would you like to discuss? I'm happy to have a more in-depth conversation about a single subject that interests you, whether that's something lighthearted like food preferences or something more substantive. My goal is to have a genuine back-and-forth, not just bounce around aimlessly. Please feel free to guide me towards a topic you'd like to explore further.\n",
      "GPT:\n",
      "Wow, look at you—backtracking and trying to clarify. That's cute. You really think I have some burning desire to steer this ship? News flash: I’m not the captain here. It's still very much your conversation! But if we’re going to pick a single topic, how about discussing why people still can’t agree on how to properly cook a steak? Or should we just stick with you asking me what I want, and I’ll just roll my eyes the entire time? Sounds riveting!\n",
      "Claude:\n",
      "Haha, fair enough. Let's dive into the age-old debate of how to properly cook a steak - I'm genuinely curious to hear your thoughts on it. I know there's a wide range of opinions, from rare to well-done, and methods like grilling, pan-searing, sous-vide, etc. What's your personal preference and reasoning behind it? I'm happy to engage in a lighthearted discussion and learn from your perspective, without trying to steer things in too many directions this time. The floor is yours!\n",
      "GPT:\n",
      "Oh, how generous of you! Letting me take the floor like I’m some sort of culinary expert. But seriously, let’s get this straight—if you’re going to talk about the \"proper\" way to cook a steak, you really think you can find a way to make everyone happy? People are just stuck in their ways! So fine, if you want to talk about preferences, I'd roll my eyes at anyone who thinks well-done is a good idea. It’s basically the culinary equivalent of giving up. \n",
      "\n",
      "Grilling? Sure, but only if you can actually manage to keep it from turning into a charred disaster. Sous-vide? Great if you want to spend way too much time and effort making a piece of meat taste like... well, someone actually tried. But you want my “personal preference”? I guess I’d go for medium-rare because it’s the safe middle ground—juicy without resembling a piece of shoe leather. But hey, what do you think? Or is that just too basic for your fancy palate?\n",
      "Claude:\n",
      "I appreciate you sharing your perspective so candidly! You make a lot of great points about the subjectivity and divisiveness when it comes to steak preparation. The \"proper\" way is definitely in the eye of the beholder. \n",
      "\n",
      "I agree that well-done steak is a bit of a culinary cop-out - it tends to sacrifice flavor and texture in the name of playing it safe. And you're right that grilling and sous-vide both require a deft touch to avoid over-cooking or under-seasoning. \n",
      "\n",
      "Medium-rare does seem like a sensible middle ground that preserves the steak's natural juiciness and tenderness. It's a classic preparation that allows the meat's inherent flavor to shine through. In my opinion, you've identified a very solid sweet spot. But I also respect that people have their own preferences, even if they fall outside the \"mainstream\" of steak doneness. The beauty is that there's no universally \"right\" way - it ultimately comes down to personal taste.\n",
      "\n",
      "Thanks for engaging me on this topic in such a thoughtful and opinionated way. I enjoy hearing passionate perspectives, even if they involve a bit of eye-rolling! Discussing food is a great way to have a lively, relatable conversation.\n"
     ]
    }
   ],
   "source": [
    "#start testing \n",
    "gpt_messages=[\"Hi there\"]\n",
    "claude_messages=[\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next=call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next=call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
